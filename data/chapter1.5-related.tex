% !Mode:: "TeX:UTF-8"
\chapter{相关技术}

本研究中利用了ResNet\cite{He2016DeepRL}、特征金字塔网络（Feature Pyramid Networks, FPN）\cite{Lin2017FeaturePN}、感兴趣区域（Region of Interest, RoI）层\cite{Girshick2015FastR,He2017MaskR}等技术。

\section{ResNet}

卷积神经网络针对网格型图像数据设计，广泛应用于计算机视觉领域，而随着多年以来卷积神经网络的发展与相应数据集的发展，ResNet\cite{He2016DeepRL}的提出极大地提升了卷积神经网络的特征提取能力。在传统的卷积神经网络中，层数更深可以使网络获得更好的非线性表达能力，但随着层数的加深，网络的收敛也变得愈发困难。这是因为由于网络较深，在网络反向传播时，经过多次计算的梯度很容易收敛到0，使该卷积核不被激活，参数不会再继续优化。因此，只有解决网络较深时的梯度丢失问题，才可以进一步提升卷积神经网络的特征提取能力。

ResNet提出了残差连接和瓶颈结构，两者皆可以用以构建更深更高效的卷积网络。前者可以解决在网络较深时出现的梯度丢失问题，后者则利用瓶颈结构使模型更小更高效。

\subsection{残差连接}

在以往的卷积网络中，采用直接堆叠卷积核的方式加深网络，如著名的VGGNet\cite{Simonyan2015VeryDC}。而在深层网络中，往往会出现梯度丢失问题，即在过多层的梯度传导中，梯度降低至0，这导致加深网络的深度反而导致网路的性能下降。因此VGGNet等往往构建不到20层卷积核便达到了最高能力。但是，本研究意图赋予模型视觉推理能力的基础是充分提取图像的像素特征，而往往较深的网络才能有满足本研究所需的特征提取能力。

而ResNet解决了这一问题，其在每个卷积的构建单元中，对残差函数增加一个捷径分支来传导梯度，防止梯度在过多卷积层的传导中丢失。这使得加深卷积网络，如使用100层以上的网络时，网络的性能依然可以继续提升。

\begin{figure}[ht]
    \centering
    \includegraphics{figure/ResNet_building_block.eps}
    \caption{一个残差连接中的卷积构建单元}
    \label{fig:resnet_building_block}
\end{figure}

残差连接的捷径分支有多种连接方法，如投影、零填充和恒等连接。

1、恒等连接

等式(\ref{res:iden})代表一个卷积构建单元，其中$\mathbf{x}$代表输入的特征，，$\mathcal{F}$代表一个由卷积核构成的残差函数，往往由两层$3 \times 3$卷积层构建而成。而$W_{i}$为该构建单元中卷积核的权重。如图\ref{fig:resnet_building_block}。

\begin{equation} \label{res:iden}
    y = \mathcal{F} (\mathbf{x}, \{W_{i}\}) + \mathbf{x}.
\end{equation}

2、投影连接

等式(\ref{res:proj})中的$W_{s}$为一个线性的投影函数，一般使用$1 \times 1$卷积核来实现。线性体现在该卷积核后不会增加激活函数，而非线性激活操作会在完成加法运算后统一进行。

\begin{equation} \label{res:proj}
    y = \mathcal{F} (\mathbf{x}, \{W_{i}\}) + W_{s} \mathbf{x}.
\end{equation}

在实际应用中ResNet在对特征图进行降维时，每个卷积构建单元的输入和输出维度不同，此时使用恒等连接会使两个维度不一样的特征向量相加，造成错误。则应使用投影连接，将单个卷积构建单元中捷径分支的维度与残差分支，即卷积核主体的维度相统一。但此时，统一特征维度所使用的卷积操作为密集连接，一定程度上会增加模型的大小。相对来说，恒等连接不会增加模型的参数和计算的复杂度。因此在卷积构建单元无降维操作时，直接使用恒等连接是更优的选择。

如ResNet-101，101层的ResNet中仅存在5次降维操作，将ImageNet的图像输入尺寸由$224 \time 224$降维到$7 \times 7$，33个卷积构建单元组成的101层卷积网络仅存在5次投影连接，即模型的大部分残差连接都由恒等连接构建而成。

3、零填充连接

零填充连接是另一种在残差分支与捷径分支维度不同时的统一维度的方法。其直接在捷径分支的输出向量要增加的维度上填充0来匹配输出，但它的效果并不如投影连接好，仅作为在运算性能受限的情况下提供的一种构建选项。

\subsection{瓶颈结构} 

我们需要更深的网络来获得更好的特征提取能力的同时，模型的参数数量也需要控制，否则训练成本就会无限制地上升。

\begin{figure}[ht]
    \centering
    \includegraphics[width=.8\textwidth]{figure/ResNet_bottleneck.eps}
    \caption{瓶颈结构的残差函数$\mathcal{F}$（右图）}
    \label{fig:resnet_bottleneck}
\end{figure}

因此，ResNet提出了瓶颈结构（Bottleneck）作为网络的卷积构建单元。如图~\ref{fig:resnet_bottleneck}，对于每个残差函数$\mathcal{F}$，其使用了三个卷积层来代替原有的两层，分别使用$1 \times 1$、$3 \times 3$和$1 \times 1$卷积核，其中$1 \times 1$卷积核负责降低然后恢复维度，使$3 \times 3$卷积核作为“瓶颈”，可以具有较小的输入和输出，大大降低了模型的参数数量和运算成本。另外，经过实验证明，应用了瓶颈结构后，模型的准确率和运算速度双双提高。

无参数的恒等连接对于瓶颈结构尤为重要。如果在瓶颈结构中全使用投影连接，会导致模型的参数数量和运算成本翻倍。这是因为捷径连接到两个$1 \times 1$卷积核的高维端，较高的特征维度若使用$1 \times 1$卷积核这样较为密集的运算，就会带来巨大的运算成本。因此恒等连接和瓶颈结构是相辅相成的设计。

\section{特征金字塔网络}

本研究意图通过以知识图谱辅助的视觉推理，提升模型在稀少样本和小物体样本上的表现，但使用卷积网络对小物体样本进行充分的特征提取仍是重中之重。小物体样本的特点是在图像上难以辨别，分辨率极低，甚至在模型降低特征图大小后，占不到一个像素。如上文提到的ResNet组成的卷积网络主干，共有五次降维操作，输出特征图大小为原分辨率的$1/32$，这意味着分辨率在$32 \times 32$以下的小物体，最终在输出特征图中只占不到一个像素。如图\ref{fig:FPN_others:l}，一些传统的目标检测模型都是这样对小物体样本进行处理的\upcite{He2014SpatialPP,Girshick2015FastR}。这导致小物体在深层网络中的特征严重不足，位置信息也几乎完全丢失，这对小样本物体的分类带来了严重困扰。

\begin{figure}[ht]
    \centering
    \subfigure[单特征图输出]{
        \includegraphics[width=0.25\textwidth]{figure/FPN_single.png}
        \label{fig:FPN_others:l}
    }
    \subfigure[特征化图像金字塔]{
        \includegraphics[width=0.40\textwidth]{figure/FPN_imageP.png}
        \label{fig:FPN_others:m}
    }
    \subfigure[金字塔型的特征层级结构]{
        \includegraphics[width=0.25\textwidth]{figure/FPN_3.png}
        \label{fig:FPN_others:r}
    }
    \caption{一些以往的解决办法图示\upcite{Lin2017FeaturePN}}
    \label{fig:FPN_others}
\end{figure}

\subsection{特征化图像金字塔} 

最初，人们使用图像金字塔解决这一问题。如图\ref{fig:FPN_others:m}，其将输入图像手动下采样到多个分辨率，从大到小，然后对每个分辨率的图像都训练一个单独的卷积网络来提取特征，生成预测信息，最终再将预测信息融合。这样的方法直接而粗暴地利用了不同分辨率下的特征信息，各个分辨率的图像不再进行降维，在原分辨率图像所在的网络下，小物体所在区域没有被缩小，则小物体的识别自然会更加简单。但这样的网络存在着很多问题，首先是巨大的计算开销，大分辨率图像所用到的网络模型本来就大，何况还要训练多个网络。另外，同时使用多个卷积网络会耗费巨大的内存，在目前计算设备的内存没有发展时，这相当于限制了单个网络的质量。

\subsection{金字塔型的特征层级结构} 

一步式多目标检测器（Single Shot MultiBox Detector, SSD）\cite{Liu2016SSDSS}是第一个使用金字塔型的特征层级结构的网络。相对于图像金字塔，一步式多目标检测器可以对不同分辨率的特征图进行多次利用，在多个特征图分辨率下加入分类器并生成预测，如图\ref{fig:FPN_others:r}。这解决了图像金字塔网络的最大弊端，大大提升了运算速度。

由于网络在较低层次时，还没有充分地提取图像特征，虽然此时有很多高分辨率的信息，但是并不能正确地对物体进行识别，因此其没有直接利用单独的高分辨率特征进行分类预测。这也说明一步式多目标检测器在预测时并没有再次利用底层高分辨率的特征，导致其不具备增强小物体分类表现的能力。这一结构遇到了如下矛盾：在深层网络的低分辨率特征，小物体的特征图太小，无法识别；而低层网络的高分辨率特征，小物体的特征还没有被充分提取，识别效果依然很差。这导致这样的结构很难提升小样本情况下的分类准确率。

这是因为一步式多目标检测器并不是针对小样本识别的难题而设计，更多的，它想提升在目标检测任务上的整体表现，但它的金字塔型结构为后来的研究奠定了基础。

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/FPN.png}
    \caption{FPN自底向上（左）和自顶向下（右）的传播路径\upcite{Lin2017FeaturePN}}
    \label{fig:FPN}
\end{figure}

\subsection{特征金字塔结构} 

特征金字塔网络解决了上文一步式多目标检测器遇到的问题。如图\ref{fig:FPN}，特征金字塔网络使用两部分分支，分别是自底向上（bottom-up）分支和自顶向下（top-down）分支。自底向上分支使用普通的卷积网络构建，它的作用就是逐层提取图像中的特征，越向上，特征提取的越完全。自顶向下分支使用了多层来自自底向上分支的特征，其连接使用$1 \times 1$卷积核实现，这可以使各个分辨率的特征更好地结合起来。而自顶向下分支中较高层的特征经过上采样后，与底层的特征使用加法操作相结合，这一步可以使底层的特征同时获得低层的高分辨率信息和高层的经过充分提取的特征信息。此时，低层特征才会生成新的预测，这样的预测就可以充分的利用各个分辨率的信息，大大提升模型对小样本的表现。特征金字塔网络的核心就是底层高分辨率特征是各个低层特征的结合，融合了多种信息，从而提升小样本的识别表现。

\section{感兴趣区域层}

\subsection{RoI Pooling} 

感兴趣区域池化层最早由Fast R-CNN\cite{Girshick2015FastR}提出，其目的是将任意大小的感兴趣区域通过最大池化（max pooling）转化成固定大小$H \times W$（如$7 \times 7$）的特征，以匹配后续全连接层的输出。在本研究中，感兴趣区域池化层用来提取对象区域的区域特征。

每一个感兴趣区域为四元组$(r, c, h, w)$，其中$(r, c)$为感兴趣区域左上角像素的坐标，$(h, w)$为感兴趣区域的长和宽。那么感兴趣区域的大小为$h \times w$。感兴趣区域池化层将$h \times w$的感兴趣区域等分成$H \times W$个窗口，则每个窗口的大小为$h / H \times w / W$。对每个窗口分别应用最大池化，得到$H \times W$个值，组合起来输出大小为$H \times W$的特征图。

但是在感兴趣区域池化层的运算过程中存在着一些量化操作，导致了计算出的区域特征和实际区域特征存在偏差。对大小为$h \times w$的感兴趣区域，其特征图大小可能因卷积网络主干的多次降维变为$h / 32 \times w / 32$，那么此时特征图的大小很可能不是整数，而感兴趣区域池化层直接对特征图坐标和大小的浮点数做了向下取整操作，这是第一次量化。另外，窗口的大小$h / H \times w / W$也可能不是整数，则此时还需要做一次取整操作，带来了第二次量化。这两次量化使区域的特征相对真实特征有小幅偏差，一定程度上影响了对物体的识别。

\subsection{RoI Align} 

随后，Mask R-CNN\cite{He2017MaskR}提出了感兴趣区域对齐层，对两次量化造成的误差做出了修正，提升了物体识别的精度。感兴趣区域对齐层不使用向下取整操作，而是直接使用精准的区域边界。由于输入的图像矩阵都是离散的，其使用了双线性插值法获取更精准的区域特征。此外，对每个区域，感兴趣区域对齐层使用了平均池化代替了最大池化。感兴趣区域对齐层相对感兴趣区域池化层取得了$3\%$的平均精度（mean AP）的绝对提升。
