% !Mode:: "TeX:UTF-8"

% 中英文摘要
\begin{cabstract}
本论文提出了一个基于GCN知识图谱分析的视觉推理模型。该模型解决了传统的基于卷积网络的目标检测模型在对图像进行分析时，仅利用图片的像素信息，难以分析图像的全局语义信息的问题，应用图卷积网络对人类常识性知识图谱中的物体关系进行分析，将图片以外的多种语义信息融入模型，从而使模型具有视觉推理的能力。该模型由两个主要模块组成，分别是CNN模块和GCN模块。其中，CNN模块由普通卷积网络结合感兴趣区域层构建而成，从而实现对图像像素特征以及图像中对象的区域特征的提取；GCN模块由图注意力层构建而成，实现区域特征在图结构的知识图谱上的传播。视觉推理模型通过结合CNN模块与GCN模块的预测结果，实现对物体的重新分类并提升区域分类准确率。其中的知识图谱构建利用到了多种关系信息，包括物体的常识性语义关系、物体属性的相关度和物体的空间关系等。相较于由最新基于卷积网络的目标检测模型Faster R-CNN构建而成的基线模型，本模型在区域分类能力方面具有更好的实验效果。在VG-1000测试集上的区域分类任务中，本模型样例平均精度(AP)比Faster R-CNN基线模型提升了$4.25\%$。
\end{cabstract}

\begin{eabstract}
In this paper, we propose a visual reasoning model based on knowledge graph analysis with GCN. This model solves the problem that the traditional object detection model based on CNN only uses the pixel information, which is difficult to analyze the global semantic information of the image. This model can analyze relationship between the objects and integrate a variety of semantic information. The model consists of two main modules, namely CNN module and GCN module. Among them, the CNN module is constructed by the plain CNN combined with the RoI layer, so as to achieve the extraction of the image pixel feature and the regional feature of the object. The GCN module is constructed by graph attention layer, and realizes the propagation of regional feature on the knowledge graph. The visual reasoning model combines the prediction result of the CNN module and the GCN module to reclassify objects and improve the accuracy of regional classification. The construction of the knowledge graph utilizes a variety of relationship information, including the human common sense semantic relationship of objects, the relevance of object attributes and the spatial relationship of objects. Compared with the baseline model constructed by the latest CNN-based simplified object detection framework Faster R-CNN, we show strong performance over the plain CNN basinline. In the regional classification task on the VG-1000 test set, the average accuracy (AP) of the samples of this model is improved by 4.25\% compared to the CNN baseline.
\end{eabstract}